# Data Directory Organization

**Last Updated**: October 10, 2025

## Purpose

This directory stores all data artifacts including trained models, evaluation results, alert logs, and performance metrics.

## Directory Structure

```
data/
├── models/                              # Trained ML models
│   ├── isolation_forest_model.pkl       # Production SNMP model
│   ├── isolation_forest_model_tuned.pkl # Hyperparameter-tuned model
│   └── isolation_forest_model_metadata.json  # Model metadata
│
├── evaluation/                          # Current evaluation results
│   ├── alerts/
│   │   └── pipeline_alerts.jsonl        # Alert logs from evaluation runs
│   ├── ground_truth/                    # Ground truth for scenarios
│   │   ├── baseline_normal_operation_ground_truth.json
│   │   ├── bgp_route_flapping_ground_truth.json
│   │   ├── hardware_degradation_ground_truth.json
│   │   ├── multimodal_correlated_failure_ground_truth.json
│   │   └── ground_truth.json
│   ├── metrics/
│   │   └── summary.json                 # Performance metrics (F1, delay, Hit@k)
│   ├── models/
│   │   └── isolation_forest_trained.pkl # Evaluation-specific model
│   ├── plots/                           # Visualizations for paper
│   ├── training/
│   │   └── snmp_training_data.jsonl     # SNMP training data
│   ├── evaluation_report.json           # Comprehensive report
│   ├── real_pipeline_results.json       # Production pipeline results
│   ├── test_run_summary.json            # Test execution summary
│   └── REAL_METRICS_SUMMARY.md          # Human-readable metrics
│
├── evaluation_coordinated/              # Coordinated failure tests (archive)
│   ├── *_ground_truth.json (6 files)
│   └── test_run_summary.json
│
├── evaluation_extended/                 # Extended evaluation tests (archive)
│   ├── *_ground_truth.json (16 files)
│   └── test_run_summary.json
│
├── alerts/                              # Live alert logs
│   └── alerts_log.jsonl
│
└── plots/                               # Latest plots for paper
    ├── confusion_matrix.png
    ├── detection_delay_cdf.png
    ├── f1_by_scenario.png
    ├── hitk_comparison.png
    └── metrics_radar.png
```

## Directory Descriptions

### models/

**Trained ML models for production use.**

- `isolation_forest_model.pkl` - Production SNMP anomaly detector
  - Trained on baseline SNMP data
  - 19-dimensional feature space
  - 5% false positive rate, 100% detection rate
  
- `isolation_forest_model_tuned.pkl` - Hyperparameter-optimized version
  - Grid search optimization
  - Better performance than base model
  
- `isolation_forest_model_metadata.json` - Model metadata
  - Feature names, training parameters, performance metrics

**Usage:**

```python
from src.models import IsolationForestDetector

detector = IsolationForestDetector()
detector.load("data/models/isolation_forest_model_tuned.pkl")
```

### evaluation/

**Current evaluation results and artifacts.**

- `alerts/` - Alert logs from evaluation runs
- `ground_truth/` - Ground truth labels for each scenario
- `metrics/` - Performance metrics (F1, Precision, Recall, etc.)
- `plots/` - Visualizations for academic paper
- `models/` - Models trained during evaluation
- `training/` - Training datasets

**Generated by**: `evaluation/run_evaluation.py`

### evaluation_coordinated/ & evaluation_extended/

**Archive of previous evaluation runs.**

These directories contain results from earlier evaluation experiments:
- Coordinated failures (6 scenarios)
- Extended evaluation suite (16 scenarios)

**Status**: Archived - kept for historical reference

### alerts/

**Live alert logs from system operation.**

- `alerts_log.jsonl` - Real-time alert log
- Format: JSON Lines (one alert per line)

**Usage**:

```bash
# View recent alerts
tail -f data/alerts/alerts_log.jsonl

# Count alerts
wc -l data/alerts/alerts_log.jsonl
```

### plots/

**Latest visualizations for academic paper.**

- `confusion_matrix.png` - Classification performance
- `detection_delay_cdf.png` - Detection delay distribution
- `f1_by_scenario.png` - F1 scores by failure type
- `hitk_comparison.png` - Localization accuracy
- `metrics_radar.png` - Multi-dimensional performance

**Generated by**: `evaluation/analyze_results.py`

## File Formats

### Ground Truth Files (.json)

```json
{
  "scenario": "link_failure",
  "failure_time": 1234567890,
  "device": "spine-01",
  "interface": "eth1",
  "failure_type": "link_down",
  "expected_alerts": ["BGP", "SNMP"]
}
```

### Alert Logs (.jsonl)

```json
{"timestamp": 1234567890, "device": "spine-01", "severity": "critical", "confidence": 0.92}
{"timestamp": 1234567895, "device": "tor-01", "severity": "warning", "confidence": 0.75}
```

### Metrics Summary (.json)

```json
{
  "f1_score": 0.89,
  "precision": 0.92,
  "recall": 0.87,
  "mean_detection_delay_sec": 28.5,
  "hit_at_1": 0.73,
  "hit_at_3": 0.91
}
```

## Data Lifecycle

### 1. Training Phase

```bash
# Generate training data
python evaluation/generate_snmp_training_data.py

# Train model
python evaluation/train_isolation_forest.py

# Output: data/models/isolation_forest_model.pkl
```

### 2. Evaluation Phase

```bash
# Run evaluation
python evaluation/run_evaluation.py

# Outputs:
# - data/evaluation/alerts/
# - data/evaluation/metrics/
# - data/evaluation/ground_truth/
```

### 3. Analysis Phase

```bash
# Analyze results
python evaluation/analyze_results.py

# Outputs:
# - data/evaluation/metrics/summary.json
# - data/plots/*.png
```

### 4. Paper Integration

Use visualizations and metrics in academic paper:
- Reference plots in LaTeX: `\includegraphics{../data/plots/f1_by_scenario.png}`
- Cite metrics in text: F1=0.89, Delay=28.5s

## Maintenance

### Cleaning Up Old Results

```bash
# Archive old evaluation runs
mkdir -p data/evaluation_archive/$(date +%Y%m%d)
mv data/evaluation/* data/evaluation_archive/$(date +%Y%m%d)/

# Or delete old results
rm -rf data/evaluation/alerts/*
rm -rf data/evaluation/metrics/*
```

### Retraining Models

```bash
# Generate fresh training data
python evaluation/generate_snmp_training_data.py

# Retrain model
python evaluation/train_isolation_forest.py

# Backup old model
cp data/models/isolation_forest_model.pkl data/models/isolation_forest_model_backup.pkl
```

## Disk Space

Current size: ~50MB (with models and plots)

- Models: ~5MB
- Evaluation results: ~10MB
- Plots: ~2MB
- Archive: ~30MB

**Tip**: Archive old evaluation runs to save space.

## Version Control

### Tracked (.git)

- `models/` - Production models (versioned)
- `plots/` - Paper figures (versioned)
- Directory structure (versioned)

### Ignored (.gitignore)

- `evaluation/alerts/*.jsonl` - Temporary logs
- `evaluation_*/` - Archived runs
- Large temporary files

## Recent Changes (October 2025)

### Cleanup ✅
- Deleted empty `lab_traces/` directory
- Deleted empty `public_traces/` directory
- Moved model files to `models/` subdirectory
- Consolidated structure

### Organization ✅
- Created clear directory hierarchy
- Documented all data formats
- Added lifecycle documentation

## Related Documentation

- Evaluation guide: `evaluation/README.md`
- Getting started: `evaluation/GETTING_STARTED.md`
- Main README: `README.md`
