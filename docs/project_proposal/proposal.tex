\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{apacite}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\title{Machine Learning for Real-Time Network Failure Detection and Localization}
\author{Michael Hernandez}
\date{September 12, 2025}

\begin{document}

% Cover Page
\begin{titlepage}
\centering
\vspace*{2cm}

{\Large \textbf{Machine Learning for Real-Time Network Failure Detection and Localization}}

\vspace{1.5cm}

{\large CUNY School of Professional Studies}

\vspace{0.5cm}

{\large Michael Hernandez}

\vspace{0.5cm}

{\large IS 499 Information Systems Capstone}

\vspace{0.5cm}

{\large Professor John Bouma}

\vspace{0.5cm}

{\large September 12, 2025}

\vfill

\end{titlepage}

\section{Topic Description}

This project builds a practical system that processes live BGP updates and device logs, using accessible machine learning to quickly detect failure-induced incidents and pinpoint likely origins (e.g., top-of-rack switch, spine/route reflector, or edge/provider device). The focus is operator value: fewer false alerts, faster action, and clear explanations of what broke and where.

\section{Problem Description}

Large BGP-routed environments generate many alarms but little guidance on what matters or where to start. SNMP and syslog thresholds flag hard failures but often over-page on benign local events and under-explain control-plane or egress faults \cite{mohammed2021}. Engineers then manually sift through logs and correlate devices, increasing detection and resolution times. In large-scale networks, this delay impacts availability and on-call workload. A system that links control-plane churn with structured log patterns and understands network role topology can cut detection time, suggest likely fault locations, and suppress noise isolated to a rack or host.

\section{Solution Approach}

The system extracts features from BGP updates (withdrawals, AS-path churn, next-hop shifts) and device logs (template counts, severity, burstiness). It applies a time-series anomaly detector to BGP data \cite{scott2024} and an unsupervised model to per-device log vectors \cite{cheng2021}. Scores are normalized, fused, and analyzed by a topology-aware localizer using a role map (server, ToR, spine/RR, edge) to identify origins and down-rank edge-local flaps \cite{tan2024}. A dashboard displays each alert with the suspected location and top signals, aiming for accuracy and clarity without heavy labeling or complex models.

\section{Coursework Foundations}

My coursework provided the foundation, while industry experience added domain depth. In Python, data structures, and databases (IS 210/211, IS 361, IS 362), I learned to build parsers, design queryable schemas, and process streaming records. Networks and infrastructure (IS 205, IS 260) gave me context to define BGP failure modes and set an SNMP/syslog baseline. Systems analysis, enterprise architectures, and project management (IS 320, IS 300, PROM 210) shaped my layered design (ingest → features → models → localization → UI), requirements, UMLs, and semester plan. Security and strategy (IS 250, IS 350) guided my use of lab-based data, secure telemetry handling, and clear communication of operator value. Where the degree built general skills, my work experience provided operational specifics like BGP-routed fabrics, anycast, VXLAN, and incident response which makes the project feasible and relevant.

\section{Development \& Evaluation Plan}

Implementation will be in Python, using libraries for streaming feature extraction and unsupervised detection, with realistic data simulators generating BGP updates and SNMP metrics. The simulated network includes two spines, two top-of-rack switches, two edges, and multiple server peers. I will test with realistic failures—link failures, router overload, hardware degradation, and route leaks—and evaluate the system's detection performance. The repository contains the dashboard, ingestion scripts, experiment notebooks, diagrams, and a LaTeX/BibTeX setup for citation and PDF export.

\subsection{Evaluation Metrics}

To show practical value, I will report three measures against the baseline. F1 reflects alert quality by balancing real failures caught (recall) with false alarms avoided (precision). Detection delay is the time from failure onset to the first alert, given as a median with interquartile range, and compared with the baseline's delay for the same runs. Hit@k measures localization accuracy by checking if the true failure origin is among the top-k suspects (e.g., Hit@1, Hit@3). These metrics capture fewer false alerts, quicker detection, and clearer first actions.

\section{Deliverables \& Timeline}

The final submission will include a paper, a presentation, a live demo with test scenarios, and a public repository with code, documentation, and weekly updates. Early work centers on creating data simulators and pipeline infrastructure, then moves to feature extraction, unsupervised scoring, topology-aware localization, and the dashboard. The final phase focuses on experiments, ablations, results packaging, and presentation polish. The scope is sized for a semester, aiming for measurable outcomes and a working proof of concept.

\section{Writing \& Formatting}

The paper will use plain, professional language and avoid jargon where possible, defining terms when needed (e.g., "BGP updates" as "routing change messages"). In-text APA citations will be included with a reference list.

% References
\bibliographystyle{apacite}
\bibliography{references}

\end{document}